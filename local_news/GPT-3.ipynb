{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torchtext import data\n",
    "import numpy as np\n",
    "import os\n",
    "from torch.nn import init\n",
    "from torchtext.vocab import Vectors\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import  train_test_split\n",
    "import math\n",
    "import logging\n",
    "import random\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import LambdaLR\n",
    "from torch.utils.data.dataloader import DataLoader\n",
    "import spacy\n",
    "import csv\n",
    "nlp = spacy.load('en')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "TEXT = data.Field(tokenize='spacy', init_token='<BOS>', eos_token='<EOS>', batch_first=True)\n",
    "LABEL = data.LabelField(use_vocab=False, sequential=False, dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_data = pd.read_csv('data/all_data_1012.tsv', sep='\\t') \n",
    "# train_data, valid_data = train_test_split(all_data, test_size = 0.2)\n",
    "# train_data.to_csv(\"./data/train_data.csv\", index=False)\n",
    "# valid_data.to_csv(\"./data/valid_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, valid_data = data.TabularDataset.splits(\n",
    "    path='./data', train='train_data.csv', test='valid_data.csv', format = 'csv',\n",
    "    fields=[('text', TEXT), ('label', LABEL)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# glove_words = data.TabularDataset(\n",
    "#     path='./data/glove_words.tsv', format = 'tsv',\n",
    "#     fields=[('word', TEXT)]\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/12/2020 13:36:44 - Loading vectors from .vector_cache/glove.6B.100d.txt.pt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "400004\n"
     ]
    }
   ],
   "source": [
    "max_vocal_size = 400000\n",
    "vectors = Vectors(name='glove.6B.100d.txt')\n",
    "TEXT.build_vocab(train_data, max_size=max_vocal_size, vectors=vectors, unk_init=torch.Tensor.normal_)\n",
    "LABEL.build_vocab(train_data)\n",
    "print(\"Vocab size: \", len(TEXT.vocab))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(TEXT.vocab.freqs.most_common(30))\n",
    "# print(TEXT.vocab.stoi['locve'])\n",
    "# index = 30005\n",
    "# print(TEXT.vocab.itos[index])\n",
    "# print(TEXT.vocab.vectors[index,:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = data.BucketIterator(\n",
    "        train_data, \n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.text), \n",
    "        sort_within_batch=False,\n",
    "        repeat=False,\n",
    "        shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_iterator = data.BucketIterator(\n",
    "        valid_data, \n",
    "        batch_size=32,\n",
    "        device=device,\n",
    "        sort_key=lambda x: len(x.text), \n",
    "        sort_within_batch=False,\n",
    "        repeat=False,\n",
    "        shuffle=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(len(train_iterator))\n",
    "# print(len(valid_iterator))\n",
    "# batch = next(iter(valid_iterator))\n",
    "# x,y = batch\n",
    "# print(sum(y==1))\n",
    "# print([TEXT.vocab.itos[int(i)] for i in batch.text[2,:512].tolist()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CausalSelfAttention(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        assert config.n_embd % config.n_head == 0\n",
    "        # key, query, value projections for all heads\n",
    "        self.key = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.query = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.value = nn.Linear(config.n_embd, config.n_embd)\n",
    "        # regularization\n",
    "        self.attn_drop = nn.Dropout(config.attn_pdrop)\n",
    "        self.resid_drop = nn.Dropout(config.resid_pdrop)\n",
    "        # output projection\n",
    "        self.proj = nn.Linear(config.n_embd, config.n_embd)\n",
    "        self.n_head = config.n_head\n",
    "\n",
    "    def forward(self, x, layer_past=None):\n",
    "        B, T, C = x.size()\n",
    "\n",
    "        # calculate query, key, values for all heads in batch and move head forward to be the batch dim\n",
    "        k = self.key(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        q = self.query(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "        v = self.value(x).view(B, T, self.n_head, C // self.n_head).transpose(1, 2) # (B, nh, T, hs)\n",
    "\n",
    "        # causal self-attention; Self-attend: (B, nh, T, hs) x (B, nh, hs, T) -> (B, nh, T, T)\n",
    "        att = (q @ k.transpose(-2, -1)) * (1.0 / math.sqrt(k.size(-1)))\n",
    "        att = F.softmax(att, dim=-1)\n",
    "        att = self.attn_drop(att)\n",
    "        y = att @ v # (B, nh, T, T) x (B, nh, T, hs) -> (B, nh, T, hs)\n",
    "        y = y.transpose(1, 2).contiguous().view(B, T, C) # re-assemble all head outputs side by side\n",
    "\n",
    "        # output projection\n",
    "        y = self.resid_drop(self.proj(y))\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Block(nn.Module):\n",
    "    \"\"\" an unassuming Transformer block \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.ln1 = nn.LayerNorm(config.n_embd)\n",
    "        self.ln2 = nn.LayerNorm(config.n_embd)\n",
    "        self.attn = CausalSelfAttention(config)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.n_embd, 4 * config.n_embd),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(4 * config.n_embd, config.n_embd),\n",
    "            nn.Dropout(config.resid_pdrop),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x + self.attn(self.ln1(x))\n",
    "        x = x + self.mlp(self.ln2(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPT(nn.Module):\n",
    "    \"\"\"  the full GPT language model, with a context size of block_size \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        \n",
    "        # input embedding stem\n",
    "        self.pad_size = config.pad_size\n",
    "        self.tok_emb = nn.Embedding(config.vocab_size, config.n_embd, padding_idx=config.pad_index)\n",
    "#         self.pos_emb = Positional_Encoding(config.n_embd, config.pad_size, config.device)\n",
    "        self.pos_emb = nn.Parameter(torch.zeros(1, config.pad_size, config.n_embd))\n",
    "        self.drop = nn.Dropout(config.embd_pdrop)\n",
    "        # transformer\n",
    "        self.blocks = nn.Sequential(*[Block(config) for _ in range(config.n_layer)])\n",
    "        # decoder head\n",
    "        self.ln_f = nn.LayerNorm(config.n_embd)\n",
    "        self.fc = nn.Linear(config.pad_size * config.n_embd, config.output_size, bias=False)\n",
    "\n",
    "        self.apply(self._init_weights)\n",
    "\n",
    "        print(\"number of parameters:\", sum(p.numel() for p in self.parameters()))\n",
    "\n",
    "\n",
    "    def _init_weights(self, module):\n",
    "        if isinstance(module, (nn.Linear, nn.Embedding)):\n",
    "            module.weight.data.normal_(mean=0.0, std=0.02)\n",
    "            if isinstance(module, nn.Linear) and module.bias is not None:\n",
    "                module.bias.data.zero_()\n",
    "        elif isinstance(module, nn.LayerNorm):\n",
    "            module.bias.data.zero_()\n",
    "            module.weight.data.fill_(1.0)\n",
    "\n",
    "    def configure_optimizers(self, train_config):\n",
    "        \n",
    "        # separate out all parameters to those that will and won't experience regularizing weight decay\n",
    "        decay = set()\n",
    "        no_decay = set()\n",
    "        whitelist_weight_modules = (torch.nn.Linear, )\n",
    "        blacklist_weight_modules = (torch.nn.LayerNorm, torch.nn.Embedding)\n",
    "        for mn, m in self.named_modules():\n",
    "            for pn, p in m.named_parameters():\n",
    "                fpn = '%s.%s' % (mn, pn) if mn else pn # full param name\n",
    "\n",
    "                if pn.endswith('bias'):\n",
    "                    # all biases will not be decayed\n",
    "                    no_decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, whitelist_weight_modules):\n",
    "                    # weights of whitelist modules will be weight decayed\n",
    "                    decay.add(fpn)\n",
    "                elif pn.endswith('weight') and isinstance(m, blacklist_weight_modules):\n",
    "                    # weights of blacklist modules will NOT be weight decayed\n",
    "                    no_decay.add(fpn)\n",
    "\n",
    "        # special case the position embedding parameter in the root GPT module as not decayed\n",
    "        no_decay.add('pos_emb')\n",
    "\n",
    "        # validate that we considered every parameter\n",
    "        param_dict = {pn: p for pn, p in self.named_parameters()}\n",
    "        inter_params = decay & no_decay\n",
    "        union_params = decay | no_decay\n",
    "        assert len(inter_params) == 0, \"parameters %s made it into both decay/no_decay sets!\" % (str(inter_params), )\n",
    "        assert len(param_dict.keys() - union_params) == 0, \"parameters %s were not separated into either decay/no_decay set!\" \\\n",
    "                                                    % (str(param_dict.keys() - union_params), )\n",
    "\n",
    "        # create the pytorch optimizer object\n",
    "        optim_groups = [\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(decay))], \"weight_decay\": train_config.weight_decay},\n",
    "            {\"params\": [param_dict[pn] for pn in sorted(list(no_decay))], \"weight_decay\": 0.0},\n",
    "        ]\n",
    "        optimizer = torch.optim.AdamW(optim_groups, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, idx, targets=None):\n",
    "        b, t = idx.size()\n",
    "#         assert t <= self.block_size, \"Cannot forward, model block size is exhausted.\"\n",
    "\n",
    "        # forward the GPT model\n",
    "        token_embeddings = self.tok_emb(idx) # each index maps to a (learnable) vector\n",
    "        position_embeddings = self.pos_emb[:, :t, :] # each position maps to a (learnable) vector\n",
    "        x = self.drop(token_embeddings + position_embeddings)\n",
    "        x = self.drop(x)\n",
    "        x = self.blocks(x)\n",
    "        x = self.ln_f(x).view(x.shape[0],-1)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        # if we are given some desired targets also calculate the loss\n",
    "        loss = None\n",
    "        if targets is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, targets)\n",
    "\n",
    "        return y_pred, loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "                if x.shape[1]<config.pad_size :\n",
    "                    padding = torch.zeros((x.shape[0],config.pad_size-x.shape[1]),dtype=torch.long).to(device)\n",
    "                    x = torch.cat([x, padding], dim=1)\n",
    "                x = x[:,:config.pad_size]\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = run_epoch('test')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    embd_pdrop = 0.1\n",
    "    resid_pdrop = 0.1\n",
    "    attn_pdrop = 0.1\n",
    "\n",
    "    def __init__(self, vocab_size, output_size, pad_index, pad_size, n_embd, **kwargs):\n",
    "        self.vocab_size = vocab_size\n",
    "        self.output_size = output_size\n",
    "        self.pad_index = pad_index\n",
    "        self.pad_size = pad_size\n",
    "        self.n_embd = n_embd\n",
    "        for k,v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'gpt-model.pt'\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# VOCAB_SIZE = len(TEXT.vocab)\n",
    "VOCAB_SIZE = max_vocal_size + 4\n",
    "OUTPUT_SIZE = 1\n",
    "# PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]\n",
    "PAD_IDX = 1\n",
    "PAD_SIZE = 512\n",
    "# EMBEDDING_SIZE = pretrained_embedding.shape[1]\n",
    "EMBEDDING_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = GPTConfig(VOCAB_SIZE, OUTPUT_SIZE, PAD_IDX, PAD_SIZE, \n",
    "                  EMBEDDING_SIZE, n_layer=4, n_head=4, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of parameters: 40588200\n"
     ]
    }
   ],
   "source": [
    "model = GPT(mconf).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([400004, 100])\n"
     ]
    }
   ],
   "source": [
    "model.tok_emb.weight.data.copy_(TEXT.vocab.vectors)\n",
    "model.tok_emb.weight.requires_grad = False\n",
    "# UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]\n",
    "# model.tok_emb.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "# model.tok_emb.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_SIZE)\n",
    "print(model.tok_emb.weight.data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT : all params: 40.588200M\n",
      "GPT : need grad params: 0.587800M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"gpt-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 4\n",
      "learning_rate 0.0006\n",
      "pad_size 512\n",
      "lr_decay True\n",
      "warmup_tokens 10240\n",
      "final_tokens 270230\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=4, learning_rate=6e-4, pad_size=PAD_SIZE,\n",
    "                      lr_decay=True, warmup_tokens=512*20, final_tokens=2*len(train_data),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_iterator, valid_iterator, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/12/2020 13:37:53 - test loss: 1.937497\n",
      "10/12/2020 13:37:53 - test score: 0.430491\n",
      "epoch 1 iter 4222: train loss 0.34098. score 0.93750. lr 3.185484e-04: 100%|██████████| 4223/4223 [06:23<00:00, 11.02it/s]\n",
      "10/12/2020 13:44:52 - test loss: 0.245965\n",
      "10/12/2020 13:44:52 - test score: 0.908319\n",
      "10/12/2020 13:44:52 - saving gpt-model.pt\n",
      "epoch 2 iter 4222: train loss 0.21289. score 0.93750. lr 6.000000e-05: 100%|██████████| 4223/4223 [06:36<00:00, 10.65it/s]\n",
      "10/12/2020 13:52:02 - test loss: 0.211380\n",
      "10/12/2020 13:52:02 - test score: 0.920101\n",
      "10/12/2020 13:52:02 - saving gpt-model.pt\n",
      "epoch 3 iter 4222: train loss 0.19892. score 0.93750. lr 3.185484e-04: 100%|██████████| 4223/4223 [06:24<00:00, 10.99it/s]\n",
      "10/12/2020 13:58:59 - test loss: 0.234490\n",
      "10/12/2020 13:58:59 - test score: 0.912019\n",
      "epoch 4 iter 4222: train loss 0.15317. score 0.90625. lr 5.977064e-04: 100%|██████████| 4223/4223 [06:26<00:00, 10.93it/s]\n",
      "10/12/2020 14:05:58 - test loss: 0.234830\n",
      "10/12/2020 14:05:58 - test score: 0.916045\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model\n",
    "    \n",
    "    def predict(self, text):\n",
    "        tokenized = [tok.text for tok in nlp.tokenizer(text)]\n",
    "        indexed = [TEXT.vocab.stoi[t] for t in tokenized]\n",
    "        x = torch.LongTensor(indexed).to(device) # seq_len\n",
    "        x = x.unsqueeze(0) # seq_len * batch_size(1)\n",
    "        if x.shape[1] < PAD_SIZE :\n",
    "            padding = torch.zeros((x.shape[0],PAD_SIZE-x.shape[1]),dtype=torch.long).to(device)\n",
    "            x = torch.cat([x, padding], dim=1)\n",
    "        x = x[:,:PAD_SIZE]\n",
    "        self.model.eval()\n",
    "        pred = torch.sigmoid(self.model(x)[0])\n",
    "        return pred.item()\n",
    "    \n",
    "    def count_acc(self, text_list, local):\n",
    "        result = []\n",
    "        for text in text_list:\n",
    "            result.append(self.predict(text))\n",
    "        result = torch.tensor(result, dtype = torch.float)\n",
    "        if local:\n",
    "            acc = sum(result > 0.5).item()/len(result)\n",
    "        else:\n",
    "            acc = sum(result < 0.5).item()/len(result)\n",
    "        return result, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "local_test = []\n",
    "with open('data/local_test.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        local_test.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "non_local_test = []\n",
    "with open('data/non_local_test.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        non_local_test.append(line[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"gpt-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result, acc = predict.count_acc(local_test, local = True)\n",
    "# acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# result_2, acc_2 = predict.count_acc(non_local_test, local = False)\n",
    "# acc_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((result < 0.5).nonzero()).squeeze()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_1 = local_test[2]\n",
    "# text_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.predict(text_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ((result_2 > 0.5).nonzero()).squeeze()[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# text_2 = non_local_test[54]\n",
    "# text_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict.predict(text_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "with open('data/test_data.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        test_text.append(line[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "fout = open('prob.tsv','w')\n",
    "for text in test_text:\n",
    "    prob = predict.predict(text)\n",
    "    fout.write('{}\\n'.format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Swift , Apple 's open source programming language , announces Swift Algorithms . Swift has announced Swift Algorithms . The announcement was made in a blog post on the Swift.org website . The algorithms should help developers fix code and improve app performance faster . Announced on Twitter on Wednesday night , Swift , Apple 's open-source programming language , is bringing new algorithm packages to developers . The announcement was made through the Swift Language account : The new open source Swift Algorithms package was just released , and ready for the community to jump in ! The new open source Swift Algorithms package was just released , and ready for the community to jump in ! To learn more about these new sequence and collection-focused algorithms , head to the https://t.co/5NNXraGyus blog : https://t.co/EsoUq1Q0pU -- Swift Language -LRB- @SwiftLang -RRB- October 8 , 2020 In a blog post on the Swift website , Nate Cook , a member of the Swift standard library team at Apple , says that the team hopes developers will adopt the new algorithms to help assist in correcting their code . `` I 'm excited to announce Swift Algorithms , a new open-source package of sequence and collection algorithms , along with their related types . Algorithms are powerful tools for thought because they encapsulate difficult-to-read and error-prone raw loops . The Algorithms package includes a host of powerful , generic algorithms frequently found in other popular programming languages . We hope this new package will help people embrace algorithms , improving the correctness and performance of their code . '' Developers can check out Swift Algorithms on the Swift website .\""
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_text[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9747289419174194"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.predict(test_text[6])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
