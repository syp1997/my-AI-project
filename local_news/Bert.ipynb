{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import csv \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import AdamW\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_data(filename):\n",
    "    text_list = []\n",
    "    label_list = []\n",
    "    f = open(filename, 'r')\n",
    "    reader = csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        text_list.append(line[0])\n",
    "        label_list.append(int(line[1]))\n",
    "    return text_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to get token ids for a list of texts \n",
    "def encode_fn(text_list):\n",
    "    all_input_ids = []    \n",
    "    for text in text_list:\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,             \n",
    "                        truncation=True,\n",
    "                        padding = 'max_length',     \n",
    "                        return_tensors = 'pt'       \n",
    "                   )\n",
    "        all_input_ids.append(input_ids)    \n",
    "    all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "    return all_input_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_data(ratio, batch_size, filename):\n",
    "    \n",
    "    text_list, label_list = prepare_data(filename)\n",
    "    all_input_ids = encode_fn(text_list)\n",
    "    labels = torch.tensor(label_list, dtype=torch.float)\n",
    "    # Split data into train and validation\n",
    "    dataset = TensorDataset(all_input_ids, labels)\n",
    "    train_size = int(ratio * len(dataset))\n",
    "    valid_size = len(dataset) - train_size\n",
    "    train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "    # Create train and validation dataloaders\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "    valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "    \n",
    "    return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "batch_size = 32\n",
    "filename = \"data/head.tsv\"\n",
    "train_dataloader, valid_dataloader = split_data(ratio, batch_size, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  250\n",
      "Num of valid_dataloader:  63\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of train_dataloader: \", len(train_dataloader))\n",
    "print(\"Num of valid_dataloader: \", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.model_name)\n",
    "        self.hidden_size = self.bert.pooler.dense.weight.shape[1]\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.fc = nn.Linear(self.hidden_size, config.output_size)\n",
    "        \n",
    "    def configure_optimizers(self, train_config):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, labels=None, token_type_ids=None, attention_mask=None):\n",
    "        _, pooled_output = self.bert(input_ids, token_type_ids, attention_mask,)\n",
    "        # pooled_output: [batch_size, dim=768]\n",
    "        x = self.dropout(pooled_output)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        # y_pred: [batch_size, output_dim]\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, labels)\n",
    "            return y_pred, loss\n",
    "        else:\n",
    "            return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    dropout_prob = 0.1\n",
    "\n",
    "    def __init__(self, output_size, model_name, **kwargs):\n",
    "        self.output_size = output_size\n",
    "        self.model_name = model_name\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = run_epoch('test')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'bert-model.pt'\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# HIDDEN_SIZE = model.bert.pooler.dense.weight.shape[1] # hidden_size = 768\n",
    "OUTPUT_SIZE = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = BertConfig(OUTPUT_SIZE, model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(mconf).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<generator object Module.parameters at 0x7f4522eaa350>"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.bert.embeddings.parameters()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in model.bert.embeddings.parameters() and model.bert.encoder.parameters():\n",
    "    par.requires_grad = False\n",
    "# for par in model.bert.encoder.parameters():\n",
    "#     par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc.weight.requires_grad = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert : all params: 109.483009M\n",
      "Bert : need grad params: 0.591361M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimizer = AdamW(model.parameters(), lr=2e-5)\n",
    "# epochs = 4\n",
    "# total_steps = len(train_dataloader) * epochs\n",
    "# scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=0, num_training_steps=total_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for epoch in range(epochs):\n",
    "#     model.train()\n",
    "#     total_loss, total_val_loss = 0, 0\n",
    "#     total_eval_accuracy = 0\n",
    "#     for it, (x,y) in enumerate(train_dataloader):\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "#         model.zero_grad()\n",
    "#         y_pred, loss = model(x, y, token_type_ids=None, attention_mask=None)\n",
    "#         total_loss += loss.item()\n",
    "#         loss.backward()\n",
    "#         torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "#         optimizer.step() \n",
    "#         scheduler.step()\n",
    "        \n",
    "#     model.eval()\n",
    "#     for it, (x,y) in enumerate(valid_dataloader):\n",
    "#         x = x.to(device)\n",
    "#         y = y.to(device)\n",
    "#         with torch.no_grad():\n",
    "#             y_pred, loss = model(x, y, token_type_ids=None, attention_mask=None)\n",
    "                \n",
    "#             total_val_loss += loss.item()\n",
    "#             total_eval_accuracy += binary_accuracy(y_pred, y)\n",
    "            \n",
    "#     avg_train_loss = total_loss / len(train_dataloader)\n",
    "#     avg_val_loss = total_val_loss / len(valid_dataloader)\n",
    "#     avg_val_accuracy = total_eval_accuracy / len(valid_dataloader)\n",
    "    \n",
    "#     print(f'Train loss     : {avg_train_loss}')\n",
    "#     print(f'Validation loss: {avg_val_loss}')\n",
    "#     print(f'Accuracy: {avg_val_accuracy:.2f}')\n",
    "#     print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 4\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 10240\n",
      "final_tokens 16000\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=4, learning_rate=6e-4,lr_decay=True, \n",
    "                      warmup_tokens=512*20, final_tokens=2*batch_size*len(train_dataloader),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/12/2020 15:57:29 - test loss: 0.675127\n",
      "10/12/2020 15:57:29 - test score: 0.582500\n",
      "epoch 1 iter 249: train loss 0.36712. score 0.87500. lr 4.687500e-04: 100%|██████████| 250/250 [01:44<00:00,  2.39it/s]\n",
      "10/12/2020 15:59:38 - test loss: 0.308867\n",
      "10/12/2020 15:59:38 - test score: 0.880500\n",
      "10/12/2020 15:59:38 - saving bert-model.pt\n",
      "epoch 2 iter 249: train loss 0.11908. score 0.96875. lr 6.000000e-05: 100%|██████████| 250/250 [01:45<00:00,  2.38it/s]\n",
      "10/12/2020 16:01:53 - test loss: 0.285198\n",
      "10/12/2020 16:01:53 - test score: 0.885000\n",
      "10/12/2020 16:01:53 - saving bert-model.pt\n",
      "epoch 3 iter 249: train loss 0.34445. score 0.90625. lr 4.026060e-04: 100%|██████████| 250/250 [01:44<00:00,  2.38it/s]\n",
      "10/12/2020 16:04:17 - test loss: 0.296193\n",
      "10/12/2020 16:04:17 - test score: 0.883000\n",
      "epoch 4 iter 249: train loss 0.23430. score 0.93750. lr 5.298133e-04: 100%|██████████| 250/250 [01:45<00:00,  2.38it/s]\n",
      "10/12/2020 16:06:26 - test loss: 0.292135\n",
      "10/12/2020 16:06:26 - test score: 0.890500\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0-pytorch1.4",
   "language": "python",
   "name": "tensorflow2.0-pytorch1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
