{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchtext.vocab import Vectors\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import collections\n",
    "from collections import Counter\n",
    "import csv \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_file = 'wiki_vector.model'\n",
    "wiki2vec = Wikipedia2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    \n",
    "    def __init__(self, data_root, text_id_root, entity_id_root, labels_root):\n",
    "        self.data_root = data_root\n",
    "        self.text_id_root = text_id_root\n",
    "        self.entity_id_root = entity_id_root\n",
    "        self.labels_root = labels_root\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        docid_list = []\n",
    "        text_list = []\n",
    "        entity_list = []\n",
    "        label_list = []\n",
    "        with open(self.data_root, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for line in reader:\n",
    "                docid_list.append(line[0])\n",
    "                text_list.append(line[1])\n",
    "                entity_list.append(list(set(line[2].split('|'))))\n",
    "                label_list.append(int(line[3]))\n",
    "        return text_list, entity_list, label_list\n",
    "\n",
    "    # Function to get token ids for a list of texts \n",
    "    def encode_text(self):\n",
    "        text_list, _, label_list = self.prepare_data()\n",
    "        all_input_ids = []    \n",
    "        num = 0\n",
    "        for text in text_list:\n",
    "            num += 1\n",
    "            if num % 10000 == 0:\n",
    "                print(num)\n",
    "            input_ids = tokenizer.encode(\n",
    "                            text,                      \n",
    "                            add_special_tokens = True,             \n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',     \n",
    "                            return_tensors = 'pt'       \n",
    "                       )\n",
    "            all_input_ids.append(input_ids)    \n",
    "        all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "        labels = torch.tensor(label_list, dtype=torch.float)\n",
    "        # Save tensor\n",
    "        torch.save(all_input_ids, self.text_id_root)\n",
    "        torch.save(labels,self.labels_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_input_ids, labels\n",
    "    \n",
    "    def encode_entity(self):\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        # get all entity\n",
    "        entity_list_all = [en for entity in entity_list for en in entity]\n",
    "        print(\"All Entity number: \", len(entity_list_all))\n",
    "        # build entity vocab\n",
    "        entity_vocab = collections.OrderedDict(Counter(entity_list_all))\n",
    "        entity_list_uniq = [entity for entity in entity_vocab.keys()]\n",
    "        entity_to_index = {entity : i+2 for i, entity in enumerate(entity_list_uniq)}\n",
    "        entity_to_index['<unk>'] = 0\n",
    "        entity_to_index['<pad>'] = 1\n",
    "        entity_to_index = collections.OrderedDict(sorted(entity_to_index.items(), key=lambda entity_to_index: entity_to_index[1]))\n",
    "        index_to_entity = [entity for i, entity in enumerate(entity_to_index)]\n",
    "        print(\"Entity vocab size: \", len(entity_to_index))\n",
    "        return entity_to_index, index_to_entity\n",
    "    \n",
    "    def build_entity_vector(self, en_embd_dim, idf_file, entity_vector_root):\n",
    "        entity_to_index, index_to_entity = self.encode_entity()\n",
    "        idf_dict, UNK_IDF = self.load_idf(idf_file)\n",
    "        # build entity vector\n",
    "        idx_to_vector=[]\n",
    "        for entity in entity_to_index.keys():\n",
    "            entity_item = wiki2vec.get_entity(entity)\n",
    "            if entity_item != None:\n",
    "                idx_to_vector.append(torch.tensor(self.normalize(wiki2vec.get_vector(entity_item))).float())\n",
    "            else:\n",
    "                words = entity.lower().split()\n",
    "                word_vectors = []\n",
    "                weights = []\n",
    "                for w in words:\n",
    "                    try:\n",
    "                        vector = wiki2vec.get_word_vector(w.lower())\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    word_vectors.append(vector)\n",
    "                    idf = idf_dict.get(w, UNK_IDF)\n",
    "                    weights.append(idf)\n",
    "                if len(word_vectors) == 0:\n",
    "                    idx_to_vector.append(torch.zeros(en_embd_dim))\n",
    "                else:\n",
    "                    word_vectors = np.array(word_vectors)\n",
    "                    weights = np.expand_dims(np.array(weights), axis=1)\n",
    "                    idx_to_vector.append(torch.tensor(self.normalize(np.sum(word_vectors * weights, axis=0))).float())\n",
    "        entity_vector = torch.stack(idx_to_vector)\n",
    "        torch.save(entity_vector, entity_vector_root)\n",
    "        return entity_vector\n",
    "        \n",
    "    def build_entity_id(self, en_pad_size):\n",
    "        # build entity index\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        all_entity_ids = []\n",
    "        for entities in entity_list:\n",
    "            entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "            for i in range(en_pad_size - len(entity_ids)):\n",
    "                entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "            all_entity_ids.append(entity_ids)\n",
    "        all_entity_ids = torch.tensor(all_entity_ids)\n",
    "        torch.save(all_entity_ids, self.entity_id_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_entity_ids\n",
    "    \n",
    "    def load_idf(self, idf_file):\n",
    "        ret = {}\n",
    "        with open(idf_file) as f:\n",
    "            for line in f:\n",
    "                phrase, count, idf = line.split('\\t')\n",
    "                idf = float(idf)\n",
    "                ret[phrase] = idf\n",
    "        return ret, ret['<UNK>']\n",
    "\n",
    "    def normalize(self, vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / (norm + 1e-9)\n",
    "        \n",
    "    def load_data(self, ratio, batch_size):\n",
    "        all_input_ids = torch.load(self.text_id_root)\n",
    "        all_entity_ids = torch.load(self.entity_id_root)\n",
    "        labels = torch.load(self.labels_root)\n",
    "        # Split data into train and validation\n",
    "        dataset = TensorDataset(all_input_ids, all_entity_ids, labels)\n",
    "        train_size = int(ratio * len(dataset))\n",
    "        valid_size = len(dataset) - train_size\n",
    "        train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "        # Create train and validation dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "        return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "batch_size = 32\n",
    "en_pad_size = 8\n",
    "en_embd_dim = 100\n",
    "idf_file = 'idf_bigram5.txt'\n",
    "data_root = \"data/all_data_1021.tsv\"\n",
    "text_id_root = \"data/text_ids_1021.pt\"\n",
    "entity_id_root = \"data/entity_ids.pt\"\n",
    "labels_root = \"data/labels_1021.pt\"\n",
    "entity_vector_root = \"data/entity_vectors.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcess(data_root, text_id_root, entity_id_root, labels_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_ids, labels = processor.encode_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity number:  1234228\n",
      "Entity vocab size:  301909\n",
      "Saved success!\n"
     ]
    }
   ],
   "source": [
    "entity_to_index, index_to_entity = processor.encode_entity()\n",
    "all_entity_ids = processor.build_entity_id(en_pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "301909"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_vector = torch.load(entity_vector_root)\n",
    "en_vocab_size = entity_vector.shape[0]\n",
    "en_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = processor.load_data(ratio, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  3743\n",
      "Num of valid_dataloader:  936\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of train_dataloader: \", len(train_dataloader))\n",
    "print(\"Num of valid_dataloader: \", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.model_name)\n",
    "        self.ln = nn.LayerNorm(self.bert.pooler.dense.weight.shape[0], eps=1e-12)\n",
    "        self.en_encoder = EntityEncoder(config)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.fc = nn.Linear(self.bert.pooler.dense.weight.shape[0]+self.en_encoder.mlp[0].weight.shape[0], config.output_size)\n",
    "        \n",
    "    def configure_optimizers(self, train_config):\n",
    "#         param_optimizer = list(model.named_parameters())  \n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#                 {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': train_config.weight_decay},\n",
    "#                 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "#         optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, entity_ids=None, labels=None, token_type_ids=None, attention_mask=None):\n",
    "        _, bert_output = self.bert(input_ids, token_type_ids, attention_mask,)\n",
    "        bert_output = self.ln(bert_output)\n",
    "        en_encoder_output = self.en_encoder(entity_ids)\n",
    "        x = torch.cat((bert_output,  en_encoder_output),dim=1)\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, labels)\n",
    "            return y_pred, loss\n",
    "        else:\n",
    "            return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEncoder(nn.Module):\n",
    "    \"\"\"Construct the embeddings from word, position and token_type embeddings.\"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.en_embeddings = nn.Embedding(config.en_vocab_size, config.en_embd_dim, padding_idx=config.pad_index)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.en_embd_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.en_embd_dim, config.en_hidden_size1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.en_hidden_size1, config.en_hidden_size2),\n",
    "#             nn.Dropout(config.dropout_prob),\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.en_hidden_size1, eps=1e-12)\n",
    "\n",
    "    def forward(self, input_ids):\n",
    "        embeddings = self.en_embeddings(input_ids)\n",
    "        \n",
    "        x = self.ln1(embeddings)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.mlp(embeddings) # batch_size * entity_num * embd_dim\n",
    "        x = F.max_pool2d(x, (x.shape[1], 1)).squeeze(1) # batch_size * embd_dim\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    dropout_prob = 0.1\n",
    "    \n",
    "    def __init__(self, output_size, model_name, en_vocab_size, en_embd_dim, en_hidden_size1, en_hidden_size2, pad_index, **kwargs):\n",
    "        self.output_size = output_size\n",
    "        self.model_name = model_name\n",
    "        self.en_vocab_size = en_vocab_size\n",
    "        self.en_embd_dim = en_embd_dim\n",
    "        self.en_hidden_size1 = en_hidden_size1\n",
    "        self.en_hidden_size2 = en_hidden_size2\n",
    "        self.pad_index = pad_index\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (text_ids, entity_ids, y) in pbar:\n",
    "                # place data on the correct device\n",
    "                text_ids = text_ids.to(self.device)\n",
    "                entity_ids = entity_ids.to(device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(text_ids, entity_ids, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = float('inf')\n",
    "#         best_loss = run_epoch('test')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'local-likely-model.pt'\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1\n",
    "en_hidden_size1 = 128\n",
    "en_hidden_size2 = 128\n",
    "pad_index = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = ModelConfig(output_size, model_name, en_vocab_size, en_embd_dim, en_hidden_size1, \n",
    "                    en_hidden_size2, pad_index,fc_hidden_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (en_encoder): EntityEncoder(\n",
       "    (en_embeddings): Embedding(301909, 100, padding_idx=0)\n",
       "    (ln1): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=896, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in model.bert.embeddings.parameters():\n",
    "    par.requires_grad = False\n",
    "for par in model.bert.encoder.layer[:11].parameters():\n",
    "    par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.en_encoder.en_embeddings.weight.data.copy_(entity_vector)\n",
    "model.en_encoder.en_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : all params: 139.705469M\n",
      "Model : need grad params: 7.710793M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 2\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 6400\n",
      "final_tokens 239552\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=2, learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=32*200, final_tokens=2*batch_size*len(train_dataloader),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 222: train loss 0.29528. score 0.90625. lr 5.999852e-04:   6%|▌         | 223/3743 [01:23<22:02,  2.66it/s]"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "    \n",
    "    def predict(self, text, entities):\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,             \n",
    "                        truncation=True,\n",
    "                        padding = 'max_length',     \n",
    "                        return_tensors = 'pt'       \n",
    "                   ).to(device)\n",
    "        \n",
    "        entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "        for i in range(en_pad_size - len(entity_ids)):\n",
    "            entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "        entity_ids = torch.tensor(entity_ids).unsqueeze(0).to(device)\n",
    "        self.model.eval()\n",
    "        pred = torch.sigmoid(self.model(input_ids, entity_ids)[0])\n",
    "        return pred.item()\n",
    "    \n",
    "    def count_acc(self, text_list, local):\n",
    "        result = []\n",
    "        for text in text_list:\n",
    "            result.append(self.predict(text))\n",
    "        result = torch.tensor(result, dtype = torch.float)\n",
    "        if local:\n",
    "            acc = sum(result > 0.5).item()/len(result)\n",
    "        else:\n",
    "            acc = sum(result < 0.5).item()/len(result)\n",
    "        return result, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load(\"local-likely-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = []\n",
    "test_entity_list = []\n",
    "with open('data/test_data_1k.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        test_text_list.append(line[0])\n",
    "        test_entity_list.append(line[1].split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = []\n",
    "for text, entities in zip(test_text_list,test_entity_list):\n",
    "    prob = predict.predict(text,entities)\n",
    "    model_predict.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(model_predict))\n",
    "fout = open('model-predict.tsv','w')\n",
    "for prob in model_predict:\n",
    "    fout.write('{}\\n'.format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Swift , Apple 's open source programming language , announces Swift Algorithms . Swift has announced Swift Algorithms . The announcement was made in a blog post on the Swift.org website . The algorithms should help developers fix code and improve app performance faster . Announced on Twitter on Wednesday night , Swift , Apple 's open-source programming language , is bringing new algorithm packages to developers . The announcement was made through the Swift Language account : The new open source Swift Algorithms package was just released , and ready for the community to jump in ! The new open source Swift Algorithms package was just released , and ready for the community to jump in ! To learn more about these new sequence and collection-focused algorithms , head to the https://t.co/5NNXraGyus blog : https://t.co/EsoUq1Q0pU -- Swift Language -LRB- @SwiftLang -RRB- October 8 , 2020 In a blog post on the Swift website , Nate Cook , a member of the Swift standard library team at Apple , says that the team hopes developers will adopt the new algorithms to help assist in correcting their code . `` I 'm excited to announce Swift Algorithms , a new open-source package of sequence and collection algorithms , along with their related types . Algorithms are powerful tools for thought because they encapsulate difficult-to-read and error-prone raw loops . The Algorithms package includes a host of powerful , generic algorithms frequently found in other popular programming languages . We hope this new package will help people embrace algorithms , improving the correctness and performance of their code . '' Developers can check out Swift Algorithms on the Swift website .\""
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 6\n",
    "test_text_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.017990360036492348"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.predict(test_text_list[index], test_entity_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
