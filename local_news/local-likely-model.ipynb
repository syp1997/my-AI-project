{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import torch.nn.utils.rnn as rnn_utils\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchtext.vocab import Vectors\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import collections\n",
    "from collections import Counter\n",
    "import csv \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# set random seed \n",
    "np.random.seed(0)\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "# confirm device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained bert model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained wiki_vector model\n",
    "model_file = 'wiki_vector.model'\n",
    "wiki2vec = Wikipedia2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    \n",
    "    def __init__(self, data_root, text_id_root, entity_id_root, entity_length_root, labels_root):\n",
    "        self.data_root = data_root\n",
    "        self.text_id_root = text_id_root\n",
    "        self.entity_id_root = entity_id_root\n",
    "        self.labels_root = labels_root\n",
    "        self.entity_length_root = entity_length_root\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        docid_list = []\n",
    "        text_list = []\n",
    "        entity_list = []\n",
    "        label_list = []\n",
    "        with open(self.data_root, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for line in reader:\n",
    "                docid_list.append(line[0])\n",
    "                text_list.append(line[1])\n",
    "                entity_list.append(list(set(line[2].split('|'))))\n",
    "                label_list.append(int(line[3]))\n",
    "        return text_list, entity_list, label_list\n",
    "\n",
    "    # Function to get token ids for a list of texts \n",
    "    def encode_text(self):\n",
    "        text_list, _, label_list = self.prepare_data()\n",
    "        all_input_ids = []    \n",
    "        num = 0\n",
    "        for text in text_list:\n",
    "            num += 1\n",
    "            if num % 10000 == 0:\n",
    "                print(num)\n",
    "            input_ids = tokenizer.encode(\n",
    "                            text,                      \n",
    "                            add_special_tokens = True,             \n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',     \n",
    "                            return_tensors = 'pt'       \n",
    "                       )\n",
    "            all_input_ids.append(input_ids)    \n",
    "        all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "        labels = torch.tensor(label_list, dtype=torch.float)\n",
    "        # Save tensor\n",
    "        torch.save(all_input_ids, self.text_id_root)\n",
    "        torch.save(labels,self.labels_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_input_ids, labels\n",
    "    \n",
    "    # Function to build entity vocab\n",
    "    def encode_entity(self):\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        # get all entity\n",
    "        entity_list_all = [en for entity in entity_list for en in entity]\n",
    "        print(\"All Entity number: \", len(entity_list_all))\n",
    "        # build entity vocab\n",
    "        entity_vocab = collections.OrderedDict(Counter(entity_list_all))\n",
    "        entity_list_uniq = [entity for entity in entity_vocab.keys()]\n",
    "        entity_to_index = {entity : i+2 for i, entity in enumerate(entity_list_uniq)}\n",
    "        entity_to_index['<unk>'] = 0\n",
    "        entity_to_index['<pad>'] = 1\n",
    "        entity_to_index = collections.OrderedDict(sorted(entity_to_index.items(), key=lambda entity_to_index: entity_to_index[1]))\n",
    "        index_to_entity = [entity for i, entity in enumerate(entity_to_index)]\n",
    "        print(\"Entity vocab size: \", len(entity_to_index))\n",
    "        return entity_to_index, index_to_entity\n",
    "    \n",
    "    # Function to build entity vocab with pretrained vector\n",
    "    def build_entity_vector(self, en_embd_dim, idf_file, entity_vector_root):\n",
    "        entity_to_index, index_to_entity = self.encode_entity()\n",
    "        idf_dict, UNK_IDF = self.load_idf(idf_file)\n",
    "        # build entity vector\n",
    "        idx_to_vector=[]\n",
    "        for entity in entity_to_index.keys():\n",
    "            entity_item = wiki2vec.get_entity(entity)\n",
    "            if entity_item != None:\n",
    "                idx_to_vector.append(torch.tensor(self.normalize(wiki2vec.get_vector(entity_item))).float())\n",
    "            else:\n",
    "                words = entity.lower().split()\n",
    "                word_vectors = []\n",
    "                weights = []\n",
    "                for w in words:\n",
    "                    try:\n",
    "                        vector = wiki2vec.get_word_vector(w.lower())\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    word_vectors.append(vector)\n",
    "                    idf = idf_dict.get(w, UNK_IDF)\n",
    "                    weights.append(idf)\n",
    "                if len(word_vectors) == 0:\n",
    "                    idx_to_vector.append(torch.zeros(en_embd_dim))\n",
    "                else:\n",
    "                    word_vectors = np.array(word_vectors)\n",
    "                    weights = np.expand_dims(np.array(weights), axis=1)\n",
    "                    idx_to_vector.append(torch.tensor(self.normalize(np.sum(word_vectors * weights, axis=0))).float())\n",
    "        entity_vector = torch.stack(idx_to_vector)\n",
    "        torch.save(entity_vector, entity_vector_root)\n",
    "        return entity_vector\n",
    "    \n",
    "    # Function to get token ids for a list of entities\n",
    "    def build_entity_id(self, en_pad_size):\n",
    "        # build entity index\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        entity_to_index, index_to_entity = self.encode_entity()\n",
    "        all_entity_ids = []\n",
    "        all_entity_length = []\n",
    "        for entities in entity_list:\n",
    "            entity_length = len(entities)\n",
    "            entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "            for i in range(en_pad_size - len(entity_ids)):\n",
    "                entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "            all_entity_ids.append(entity_ids)\n",
    "            all_entity_length.append(entity_length)\n",
    "        all_entity_ids = torch.tensor(all_entity_ids)\n",
    "        all_entity_length = torch.tensor(all_entity_length)\n",
    "        torch.save(all_entity_ids, self.entity_id_root)\n",
    "        torch.save(all_entity_length, self.entity_length_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_entity_ids, all_entity_length\n",
    "    \n",
    "    # load idf file\n",
    "    def load_idf(self, idf_file):\n",
    "        ret = {}\n",
    "        with open(idf_file) as f:\n",
    "            for line in f:\n",
    "                phrase, count, idf = line.split('\\t')\n",
    "                idf = float(idf)\n",
    "                ret[phrase] = idf\n",
    "        return ret, ret['<UNK>']\n",
    "\n",
    "    # normlize entity vector\n",
    "    def normalize(self, vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / (norm + 1e-9)\n",
    "    \n",
    "    # build dataset and dataloader\n",
    "    def load_data(self, ratio, batch_size):\n",
    "        all_input_ids = torch.load(self.text_id_root)\n",
    "        all_entity_ids = torch.load(self.entity_id_root)\n",
    "        all_entity_length = torch.load(self.entity_length_root)\n",
    "        labels = torch.load(self.labels_root)\n",
    "        # Split data into train and validation\n",
    "        dataset = TensorDataset(all_input_ids, all_entity_ids, all_entity_length, labels)\n",
    "        train_size = int(ratio * len(dataset))\n",
    "        valid_size = len(dataset) - train_size\n",
    "        train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "        # Create train and validation dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "        return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 # ratio of train data to valid data\n",
    "batch_size = 32 # batch size\n",
    "en_pad_size = 12 # max entity number of one data\n",
    "en_embd_dim = 100 # entity embedding dim\n",
    "idf_file = 'idf_bigram5.txt'\n",
    "data_root = \"data/all_data_1022.tsv\" # data: docid, text, entities, label\n",
    "text_id_root = \"data/text_ids_1022.pt\" # data_size * 512\n",
    "entity_id_root = \"data/entity_ids.pt\" # data_size * 12\n",
    "entity_length_root = \"data/entity_length.pt\" # data_size\n",
    "labels_root = \"data/labels_1022.pt\" # data_size\n",
    "entity_vector_root = \"data/entity_vectors.pt\" # en_vocab_size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcess(data_root, text_id_root, entity_id_root, entity_length_root, labels_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this when using new data\n",
    "# all_input_ids, labels = processor.encode_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity number:  4174923\n",
      "Entity vocab size:  706084\n"
     ]
    }
   ],
   "source": [
    "# get entity vocab for predict\n",
    "entity_to_index, index_to_entity = processor.encode_entity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # run this when use new data\n",
    "# # build_entity_vector = processor.build_entity_vector(en_embd_dim, idf_file, entity_vector_root)\n",
    "# all_entity_ids, all_entity_length = processor.build_entity_id(en_pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "706084"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "entity_vector = torch.load(entity_vector_root) # get pretrained entity_vector\n",
    "en_vocab_size = entity_vector.shape[0] # get entity vocab size\n",
    "en_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = processor.load_data(ratio, batch_size) # build train/valid dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  12694\n",
      "Num of valid_dataloader:  3174\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of train_dataloader: \", len(train_dataloader))\n",
    "print(\"Num of valid_dataloader: \", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.model_name)\n",
    "        self.ln = nn.LayerNorm(self.bert.pooler.dense.weight.shape[0], eps=1e-12)\n",
    "        self.use_en_encoder = config.use_en_encoder\n",
    "        if self.use_en_encoder: # if use entity infomation\n",
    "            self.en_encoder = EntityEncoder(config)\n",
    "            self.dropout = nn.Dropout(config.dropout_prob)\n",
    "            self.fc = nn.Linear(self.bert.pooler.dense.weight.shape[0]+self.en_encoder.mlp[0].weight.shape[0], config.output_size)\n",
    "        else:\n",
    "            self.fc = nn.Linear(self.bert.pooler.dense.weight.shape[0], config.output_size)\n",
    "        \n",
    "    def configure_optimizers(self, train_config):\n",
    "#         # use weight decay to optimize\n",
    "#         param_optimizer = list(model.named_parameters())  \n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#                 {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': train_config.weight_decay},\n",
    "#                 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "#         optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, entity_ids, entity_length, labels=None, token_type_ids=None, attention_mask=None):\n",
    "        _, bert_output = self.bert(input_ids, token_type_ids, attention_mask,)\n",
    "        bert_output = self.ln(bert_output)\n",
    "        if self.use_en_encoder: # if use entity infomation\n",
    "            en_encoder_output = self.en_encoder(entity_ids, entity_length)\n",
    "            x = torch.cat((bert_output,  en_encoder_output),dim=1)\n",
    "        else:\n",
    "            x = bert_output\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        \n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, labels)\n",
    "            return y_pred, loss\n",
    "        else:\n",
    "            return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEncoder(nn.Module):\n",
    "    \"\"\" Encode entities to generate single presentation \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.en_embeddings = nn.Embedding(config.en_vocab_size, config.en_embd_dim, padding_idx=config.pad_index)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.en_embd_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.en_embd_dim, config.en_hidden_size1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.en_hidden_size1, config.en_hidden_size2),\n",
    "#             nn.Dropout(config.dropout_prob), # maybe useful\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.en_hidden_size1, eps=1e-12)\n",
    "\n",
    "    def forward(self, entity_ids, entity_length):\n",
    "        embeddings = self.en_embeddings(entity_ids)\n",
    "        \n",
    "        x = self.ln1(embeddings)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.mlp(embeddings) # batch_size * entity_num * embd_dim\n",
    "        x = self.single_pool(x, entity_length) #batch_size * embd_dim\n",
    "        x = self.ln2(x)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # do this because of different entity length\n",
    "    def single_pool(self, x, x_length):\n",
    "        all_pool_out = []\n",
    "        for i in range(x.shape[0]):\n",
    "            if x_length[i] == 0:\n",
    "                 x_length[i] += 1\n",
    "            single_data = x[i][:x_length[i]].unsqueeze(0)\n",
    "            pool_out = F.max_pool2d(single_data, (single_data.shape[1], 1)).squeeze(1)\n",
    "            all_pool_out.append(pool_out)\n",
    "        x = torch.cat(all_pool_out,dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\" base mdoel config \"\"\"\n",
    "    dropout_prob = 0.1\n",
    "    \n",
    "    def __init__(self, output_size, model_name, en_vocab_size, en_embd_dim, en_hidden_size1, en_hidden_size2, pad_index, **kwargs):\n",
    "        self.output_size = output_size\n",
    "        self.model_name = model_name\n",
    "        self.en_vocab_size = en_vocab_size\n",
    "        self.en_embd_dim = en_embd_dim\n",
    "        self.en_hidden_size1 = en_hidden_size1\n",
    "        self.en_hidden_size2 = en_hidden_size2\n",
    "        self.pad_index = pad_index\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (text_ids, entity_ids, entity_length, y) in pbar:\n",
    "                # place data on the correct device\n",
    "                text_ids = text_ids.to(self.device)\n",
    "                entity_ids = entity_ids.to(device)\n",
    "                entity_length = entity_length.to(device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(text_ids, entity_ids, entity_length, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # may useful optimize method\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False # optimize method\n",
    "    warmup_tokens = 375e6 # use this to train model from a lower learning rate\n",
    "    final_tokens = 260e9 # all tokens during whole training process\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'local-likely-model.pt' # save model path\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1 # local(1) or non-local(0)\n",
    "en_hidden_size1 = 128 # hyper parameter\n",
    "en_hidden_size2 = 128 # hyper parameter\n",
    "pad_index = entity_to_index[\"<pad>\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = ModelConfig(output_size, model_name, en_vocab_size, en_embd_dim, en_hidden_size1, \n",
    "                    en_hidden_size2, pad_index, fc_hidden_size=512, use_en_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Model(\n",
       "  (bert): BertModel(\n",
       "    (embeddings): BertEmbeddings(\n",
       "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
       "      (position_embeddings): Embedding(512, 768)\n",
       "      (token_type_embeddings): Embedding(2, 768)\n",
       "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "      (dropout): Dropout(p=0.1, inplace=False)\n",
       "    )\n",
       "    (encoder): BertEncoder(\n",
       "      (layer): ModuleList(\n",
       "        (0): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (1): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (2): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (3): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (4): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (5): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (6): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (7): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (8): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (9): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (10): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "        (11): BertLayer(\n",
       "          (attention): BertAttention(\n",
       "            (self): BertSelfAttention(\n",
       "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "            (output): BertSelfOutput(\n",
       "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "            )\n",
       "          )\n",
       "          (intermediate): BertIntermediate(\n",
       "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
       "          )\n",
       "          (output): BertOutput(\n",
       "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
       "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (pooler): BertPooler(\n",
       "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
       "      (activation): Tanh()\n",
       "    )\n",
       "  )\n",
       "  (ln): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
       "  (en_encoder): EntityEncoder(\n",
       "    (en_embeddings): Embedding(706084, 100, padding_idx=1)\n",
       "    (ln1): LayerNorm((100,), eps=1e-12, elementwise_affine=True)\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "    (mlp): Sequential(\n",
       "      (0): Linear(in_features=100, out_features=128, bias=True)\n",
       "      (1): GELU()\n",
       "      (2): Linear(in_features=128, out_features=128, bias=True)\n",
       "    )\n",
       "    (ln2): LayerNorm((128,), eps=1e-12, elementwise_affine=True)\n",
       "  )\n",
       "  (dropout): Dropout(p=0.1, inplace=False)\n",
       "  (fc): Linear(in_features=896, out_features=1, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# print model structure\n",
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not train bert embedding layer\n",
    "for par in model.bert.embeddings.parameters(): \n",
    "    par.requires_grad = False\n",
    "# only train last(11th) bert encode layer\n",
    "for par in model.bert.encoder.layer[:11].parameters(): \n",
    "    par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load pretrain entity vector\n",
    "model.en_encoder.en_embeddings.weight.data.copy_(entity_vector)\n",
    "# do not train entity embedding layer\n",
    "model.en_encoder.en_embeddings.weight.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : all params: 180.122969M\n",
      "Model : need grad params: 7.710793M\n"
     ]
    }
   ],
   "source": [
    "# print model all parameters and parameters need training\n",
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 2\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 6400\n",
      "final_tokens 812416\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=2, learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=32*200, final_tokens=2*batch_size*len(train_dataloader),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 12693: train loss 0.14216. score 0.93103. lr 3.037452e-04: 100%|██████████| 12694/12694 [1:22:05<00:00,  2.58it/s]\n",
      "10/22/2020 04:02:41 - test loss: 0.080864\n",
      "10/22/2020 04:02:41 - test score: 0.972241\n",
      "10/22/2020 04:02:41 - saving local-likely-model.pt\n",
      "epoch 2 iter 12693: train loss 0.00602. score 1.00000. lr 6.000000e-05: 100%|██████████| 12694/12694 [1:22:09<00:00,  2.57it/s]\n",
      "10/22/2020 05:41:28 - test loss: 0.070388\n",
      "10/22/2020 05:41:28 - test score: 0.975421\n",
      "10/22/2020 05:41:28 - saving local-likely-model.pt\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "    \n",
    "    def predict(self, text, entities):\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,             \n",
    "                        truncation=True,\n",
    "                        padding = 'max_length',     \n",
    "                        return_tensors = 'pt'       \n",
    "                   ).to(device)\n",
    "        \n",
    "        entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "        for i in range(en_pad_size - len(entity_ids)):\n",
    "            entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "        entity_length = torch.tensor(len(entities)).unsqueeze(0).to(device)    \n",
    "        entity_ids = torch.tensor(entity_ids).unsqueeze(0).to(device)\n",
    "        \n",
    "        self.model.eval()\n",
    "        pred = torch.sigmoid(self.model(input_ids, entity_ids, entity_length)[0])\n",
    "        return pred.item()\n",
    "    \n",
    "    def count_acc(self, text_list, local):\n",
    "        result = []\n",
    "        for text in text_list:\n",
    "            result.append(self.predict(text))\n",
    "        result = torch.tensor(result, dtype = torch.float)\n",
    "        if local:\n",
    "            acc = sum(result > 0.5).item()/len(result)\n",
    "        else:\n",
    "            acc = sum(result < 0.5).item()/len(result)\n",
    "        return result, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"local-likely-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = []\n",
    "test_entity_list = []\n",
    "with open('data/test_data_1k.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        test_text_list.append(line[0])\n",
    "        test_entity_list.append(line[1].split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = []\n",
    "for text, entities in zip(test_text_list,test_entity_list):\n",
    "    prob = predict.predict(text,entities)\n",
    "    model_predict.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(model_predict))\n",
    "fout = open('model-predict.tsv','w')\n",
    "for prob in model_predict:\n",
    "    fout.write('{}\\n'.format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"REPORT : Goran Dragic active for Heat in Game 6 of NBA Finals vs. Lakers . Miami Heat guard Goran Dragic has been listed as active for Game 6 of the NBA Finals against the Los Angeles Lakers . Dragic missed the past four Finals games after suffering a torn plantar fascia in Game 1 . There was very little hope that he 'd be able to play again in the series , but it seems he has found a way to return in a crucial contest for the Heat . Miami 's Goran Dragic is listed active for Game 6 vs. Lakers . -- Shams Charania -LRB- @ShamsCharania -RRB- October 11 , 2020 The Slovenian playmaker has been a huge part of the Heat 's successful run to the Finals . Prior to getting injured in the championship opener , he was averaging 20.9 points , 4.2 rebounds and 4.7 assists in the 2020 postseason . He was also efficient for Miami , shooting 45.2 percent from the field and 36.3 percent from deep . After he went down in Game 1 alongside Bam Adebayo , many fans thought it 's over for him and the Heat . After all , his injury is something that is not easy to get back to . Moreover , several experts also believed the Lakers would be able to close out the series quickly considering the situation . Fortunately for Dragic , the Heat were able to make the Finals a series after going down 2-0 against the Lakers . The Purple and Gold still lead , but Miami has a fighting chance after winning Game 5 and cutting the lead to just 3-2 . Of course , being active does n't mean Goran Dragic will be playing since Erik Spoelstra and Co. will still determine if the risk is worth it . However , his presence alone is enough to motivate the South Beach franchise as they look to extend the Finals to a do-or-die Game 7 . For what it 's worth , Dragic has been adamant about his desire to play despite the injury . Before Game 4 , he tried shooting during warm-ups as he assessed his pain tolerance . He was initially listed as doubtful for Game 6 , but the long rest he got seemed to have played a huge role in his comeback .\""
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 29\n",
    "test_text_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6861947774887085"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.predict(test_text_list[index], test_entity_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
