{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import transformers\n",
    "from transformers import BertModel, BertTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "from torchtext.vocab import Vectors\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import collections\n",
    "from collections import Counter\n",
    "import csv \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "# set random seed \n",
    "np.random.seed(0) \n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu') # confirm device\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained bert model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained wiki_vector model\n",
    "model_file = '/data/suyinpei/wiki_vector.model'\n",
    "wiki2vec = Wikipedia2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    \n",
    "    def __init__(self, data_root, text_id_root, labels_root, entity_id_root, entity_length_root, entity_score_root):\n",
    "        self.data_root = data_root\n",
    "        self.text_id_root = text_id_root\n",
    "        self.labels_root = labels_root\n",
    "        self.entity_id_root = entity_id_root\n",
    "        self.entity_length_root = entity_length_root\n",
    "        self.entity_score_root = entity_score_root\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        docid_list = []\n",
    "        text_list = []\n",
    "        entity_list = []\n",
    "        label_list = []\n",
    "        with open(self.data_root, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for line in reader:\n",
    "                docid_list.append(line[0])\n",
    "                text_list.append(line[1])\n",
    "                entity_list.append(list(set(line[2].split('|'))))\n",
    "                label_list.append(int(line[3]))\n",
    "        return text_list, entity_list, label_list\n",
    "\n",
    "    # Function to get token ids for a list of texts \n",
    "    def encode_text(self):\n",
    "        text_list, _, label_list = self.prepare_data()\n",
    "        all_input_ids = []    \n",
    "        num = 0\n",
    "        for text in text_list:\n",
    "            num += 1\n",
    "            if num % 10000 == 0:\n",
    "                print(num)\n",
    "            input_ids = tokenizer.encode(\n",
    "                            text,                      \n",
    "                            add_special_tokens = True,             \n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',     \n",
    "                            return_tensors = 'pt'       \n",
    "                       )\n",
    "            all_input_ids.append(input_ids)    \n",
    "        all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "        labels = torch.tensor(label_list, dtype=torch.float)\n",
    "        # Save tensor\n",
    "        torch.save(all_input_ids, self.text_id_root)\n",
    "        torch.save(labels,self.labels_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_input_ids, labels\n",
    "    \n",
    "    # Function to build entity vocab\n",
    "    def encode_entity(self):\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        # get all entity\n",
    "        entity_list_all = [en for entity in entity_list for en in entity]\n",
    "        print(\"All Entity number: \", len(entity_list_all))\n",
    "        # build entity vocab\n",
    "        entity_vocab = collections.OrderedDict(Counter(entity_list_all))\n",
    "        entity_list_uniq = [entity for entity in entity_vocab.keys()]\n",
    "        entity_to_index = {entity : i+2 for i, entity in enumerate(entity_list_uniq)}\n",
    "        entity_to_index['<unk>'] = 0\n",
    "        entity_to_index['<pad>'] = 1\n",
    "        entity_to_index = collections.OrderedDict(sorted(entity_to_index.items(), key=lambda entity_to_index: entity_to_index[1]))\n",
    "        index_to_entity = [entity for i, entity in enumerate(entity_to_index)]\n",
    "        print(\"Entity vocab size: \", len(entity_to_index))\n",
    "        return entity_to_index, index_to_entity\n",
    "    \n",
    "    # Function to build entity vocab with pretrained vector\n",
    "    def build_entity_vector(self, en_embd_dim, idf_file, entity_vector_root):\n",
    "        entity_to_index, index_to_entity = self.encode_entity()\n",
    "        idf_dict, UNK_IDF = self.load_idf(idf_file)\n",
    "        # build entity vector\n",
    "        idx_to_vector=[]\n",
    "        for entity in entity_to_index.keys():\n",
    "            entity_item = wiki2vec.get_entity(entity)\n",
    "            if entity_item != None:\n",
    "                idx_to_vector.append(torch.tensor(self.en_vector_norm(wiki2vec.get_vector(entity_item))).float())\n",
    "            else:\n",
    "                words = entity.lower().split()\n",
    "                word_vectors = []\n",
    "                weights = []\n",
    "                for w in words:\n",
    "                    try:\n",
    "                        vector = wiki2vec.get_word_vector(w.lower())\n",
    "                    except KeyError:\n",
    "                        continue\n",
    "                    word_vectors.append(vector)\n",
    "                    idf = idf_dict.get(w, UNK_IDF)\n",
    "                    weights.append(idf)\n",
    "                if len(word_vectors) == 0:\n",
    "                    idx_to_vector.append(torch.zeros(en_embd_dim))\n",
    "                else:\n",
    "                    word_vectors = np.array(word_vectors)\n",
    "                    weights = np.expand_dims(np.array(weights), axis=1)\n",
    "                    idx_to_vector.append(torch.tensor(self.en_vector_norm(np.sum(word_vectors * weights, axis=0))).float())\n",
    "        entity_vector = torch.stack(idx_to_vector)\n",
    "        torch.save(entity_vector, entity_vector_root)\n",
    "        print(\"Saved success!\")\n",
    "        return entity_vector\n",
    "    \n",
    "    # Function to get token ids for a list of entities\n",
    "    def build_entity_id(self, en_pad_size):\n",
    "        # build entity index\n",
    "        _, entity_list, _ = self.prepare_data()\n",
    "        entity_to_index, index_to_entity = self.encode_entity()\n",
    "        entity_score_dict = self.load_entity_score_dict()\n",
    "        all_entity_ids = []\n",
    "        all_entity_length = []\n",
    "        all_entity_score = []\n",
    "        for entities in entity_list:\n",
    "            entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "            for i in range(en_pad_size - len(entity_ids)):\n",
    "                entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "            all_entity_ids.append(entity_ids)\n",
    "            # record entity length\n",
    "            all_entity_length.append(len(entities))\n",
    "            # build entity score\n",
    "            entity_score = []\n",
    "            score = 1\n",
    "            for en in entities:\n",
    "                if en in entity_score_dict:\n",
    "                    en_score = float(entity_score_dict[en])\n",
    "                    score *= en_score\n",
    "            score = math.log(score,10)\n",
    "            entity_score.append(score)\n",
    "            if score >= 0:\n",
    "                entity_score.append(score**2)\n",
    "                entity_score.append(score**0.5)\n",
    "            else:\n",
    "                entity_score.append(-score**2)\n",
    "                entity_score.append(-(abs(score)**0.5))\n",
    "            all_entity_score.append(entity_score)\n",
    "        all_entity_ids = torch.tensor(all_entity_ids)\n",
    "        all_entity_length = torch.tensor(all_entity_length)\n",
    "        all_entity_score = torch.tensor(all_entity_score)\n",
    "        torch.save(all_entity_ids, self.entity_id_root)\n",
    "        torch.save(all_entity_length, self.entity_length_root)\n",
    "        torch.save(self.en_score_norm(all_entity_score), self.entity_score_root)\n",
    "        print(\"Saved success!\")\n",
    "        return all_entity_ids, all_entity_length, all_entity_score\n",
    "    \n",
    "    # load idf file\n",
    "    def load_idf(self, idf_file):\n",
    "        ret = {}\n",
    "        with open(idf_file) as f:\n",
    "            for line in f:\n",
    "                phrase, count, idf = line.split('\\t')\n",
    "                idf = float(idf)\n",
    "                ret[phrase] = idf\n",
    "        return ret, ret['<UNK>']\n",
    "    \n",
    "    def load_entity_score_dict(self):\n",
    "        entity_score_dict = {}\n",
    "        with open(\"entity_frep.tsv\") as f:\n",
    "            for line in f:\n",
    "                entity, c1, c2, freq = line.split('\\t')\n",
    "                c1 = int(c1)\n",
    "                c2 = int(c2)\n",
    "                if c1 == 0 or c2 == 0:\n",
    "                    c1 += 1\n",
    "                    c2 += 1\n",
    "                if c1 + c2 > 10:\n",
    "                    entity_score_dict[entity] = freq\n",
    "        print(\"Entity Score vocab size: \", len(entity_score_dict))\n",
    "        return entity_score_dict\n",
    "        \n",
    "    # normlize entity vector\n",
    "    def en_vector_norm(self, vector):\n",
    "        norm = np.linalg.norm(vector)\n",
    "        return vector / (norm + 1e-9)\n",
    "    \n",
    "    def en_score_norm(self,x):\n",
    "        mean = x.mean(dim=0,keepdim=True)\n",
    "        std = x.std(dim=0, unbiased=False,keepdim=True)\n",
    "        x_norm = (x - mean)/std\n",
    "        return x_norm\n",
    "    \n",
    "    # build dataset and dataloader\n",
    "    def load_data(self, ratio, batch_size):\n",
    "        all_input_ids = torch.load(self.text_id_root)\n",
    "        all_entity_ids = torch.load(self.entity_id_root)\n",
    "        all_entity_length = torch.load(self.entity_length_root)\n",
    "        all_entity_score = torch.load(self.entity_score_root)\n",
    "        labels = torch.load(self.labels_root)\n",
    "        # Split data into train and validation\n",
    "        dataset = TensorDataset(all_input_ids, all_entity_ids, all_entity_length, all_entity_score, labels)\n",
    "        train_size = int(ratio * len(dataset))\n",
    "        valid_size = len(dataset) - train_size\n",
    "        train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "        # Create train and validation dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = batch_size, shuffle = True)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = batch_size, shuffle = False)\n",
    "\n",
    "        return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 # ratio of train data to valid data\n",
    "batch_size = 32 # batch size\n",
    "en_pad_size = 12 # max entity number of one data\n",
    "en_embd_dim = 100 # entity embedding dim\n",
    "idf_file = '/data/suyinpei/idf_bigram5.txt'\n",
    "data_root = \"/data/suyinpei/all_data_1026.tsv\" # data: docid, text, entities, label\n",
    "text_id_root = \"data/text_ids_1026.pt\" # data_size * 512\n",
    "labels_root = \"data/labels_1026.pt\" # data_size\n",
    "entity_id_root = \"data/entity_ids_1026.pt\" # data_size * 12\n",
    "entity_length_root = \"data/entity_length_1026.pt\" # data_size\n",
    "entity_score_root = \"data/entity_score_1026.pt\" # data_size * 3\n",
    "entity_vector_root = \"data/entity_vectors_1026.pt\" # en_vocab_size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcess(data_root, text_id_root, labels_root, entity_id_root, entity_length_root, entity_score_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this when using new data\n",
    "# all_input_ids, labels = processor.encode_text()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity number:  7684627\n",
      "Entity vocab size:  1586333\n"
     ]
    }
   ],
   "source": [
    "# get entity vocab for predict\n",
    "entity_to_index, index_to_entity = processor.encode_entity()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Score vocab size:  196714\n"
     ]
    }
   ],
   "source": [
    "# get entity score dict \n",
    "entity_score_dict = processor.load_entity_score_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity number:  7684627\n",
      "Entity vocab size:  1586333\n",
      "Entity Score vocab size:  196714\n",
      "Saved success!\n"
     ]
    }
   ],
   "source": [
    "# # run this when use new data\n",
    "# build_entity_vector = processor.build_entity_vector(en_embd_dim, idf_file, entity_vector_root)\n",
    "all_entity_ids, all_entity_length, all_entity_score = processor.build_entity_id(en_pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "entity_vector = torch.load(entity_vector_root) # get pretrained entity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = processor.load_data(ratio, batch_size) # build train/valid dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  12646\n",
      "Num of valid_dataloader:  3162\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of train_dataloader: \", len(train_dataloader))\n",
    "print(\"Num of valid_dataloader: \", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = BertModel.from_pretrained(config.model_name)\n",
    "        self.ln = nn.LayerNorm(self.bert.pooler.dense.weight.shape[0], eps=1e-12)\n",
    "        self.use_en_encoder = config.use_en_encoder\n",
    "        if self.use_en_encoder: # if use entity infomation\n",
    "            self.en_encoder = EntityEncoder(config)\n",
    "            self.dropout = nn.Dropout(config.dropout_prob)\n",
    "            self.fc = nn.Linear(self.bert.pooler.dense.weight.shape[0]+self.en_encoder.mlp[0].weight.shape[0]+config.en_score_dim, config.output_size)\n",
    "        else:\n",
    "            self.dropout = nn.Dropout(config.dropout_prob)\n",
    "            self.fc = nn.Linear(self.bert.pooler.dense.weight.shape[0], config.output_size)\n",
    "        \n",
    "    def configure_optimizers(self, train_config):\n",
    "#         # use weight decay to optimize\n",
    "#         param_optimizer = list(model.named_parameters())  \n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#                 {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': train_config.weight_decay},\n",
    "#                 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "#         optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, entity_ids, entity_length, entity_score, labels=None, token_type_ids=None, attention_mask=None):\n",
    "        _, bert_output = self.bert(input_ids, token_type_ids, attention_mask,)\n",
    "        bert_output = self.ln(bert_output)\n",
    "        if self.use_en_encoder: # if use entity infomation\n",
    "            en_encoder_output = self.en_encoder(entity_ids, entity_length, entity_score)\n",
    "            x = torch.cat((bert_output,  en_encoder_output),dim=1)\n",
    "        else:\n",
    "            x = bert_output\n",
    "        x = self.dropout(x)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, labels)\n",
    "            return y_pred, loss\n",
    "        else:\n",
    "            return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EntityEncoder(nn.Module):\n",
    "    \"\"\" Encode entities to generate single presentation \"\"\"\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.en_embeddings = nn.Embedding.from_pretrained(config.entity_vector,freeze=True)\n",
    "\n",
    "        self.ln1 = nn.LayerNorm(config.en_embd_dim, eps=1e-12)\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.mlp = nn.Sequential(\n",
    "            nn.Linear(config.en_embd_dim, config.en_hidden_size1),\n",
    "            nn.GELU(),\n",
    "            nn.Linear(config.en_hidden_size1, config.en_hidden_size2),\n",
    "#             nn.Dropout(config.dropout_prob), # maybe useful\n",
    "        )\n",
    "        self.ln2 = nn.LayerNorm(config.en_hidden_size1, eps=1e-12)\n",
    "\n",
    "    def forward(self, entity_ids, entity_length, entity_score):\n",
    "        embeddings = self.en_embeddings(entity_ids)\n",
    "        \n",
    "        x = self.ln1(embeddings)\n",
    "        x = self.dropout(x)\n",
    "        \n",
    "        x = self.mlp(embeddings) # batch_size * entity_num * embd_dim\n",
    "        x = self.single_pool(x, entity_length) #batch_size * embd_dim\n",
    "        x = self.ln2(x)\n",
    "        x = torch.cat((x, entity_score),dim=1)\n",
    "        \n",
    "        return x\n",
    "    \n",
    "    # do this because of different entity length\n",
    "    def single_pool(self, x, x_length):\n",
    "        all_pool_out = []\n",
    "        for i in range(x.shape[0]):\n",
    "            if x_length[i] == 0:\n",
    "                 x_length[i] += 1\n",
    "            single_data = x[i][:x_length[i]].unsqueeze(0)\n",
    "            pool_out = F.max_pool2d(single_data, (single_data.shape[1], 1)).squeeze(1)\n",
    "            all_pool_out.append(pool_out)\n",
    "        x = torch.cat(all_pool_out,dim=0)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (text_ids, entity_ids, entity_length, entity_score, y) in pbar:\n",
    "                # place data on the correct device\n",
    "                text_ids = text_ids.to(self.device)\n",
    "                entity_ids = entity_ids.to(device)\n",
    "                entity_length = entity_length.to(device)\n",
    "                entity_score = entity_score.to(device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(text_ids, entity_ids, entity_length, entity_score, labels=y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = float('inf')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConfig:\n",
    "    \"\"\" base mdoel config \"\"\"\n",
    "    output_size = 1 # local(1) or non-local(0)\n",
    "    dropout_prob = 0.1\n",
    "    \n",
    "    def __init__(self, model_name, entity_vector, en_embd_dim, en_hidden_size1, en_hidden_size2, \n",
    "                 en_score_dim, **kwargs):\n",
    "        self.model_name = model_name\n",
    "        self.entity_vector = entity_vector\n",
    "        self.en_embd_dim = en_embd_dim\n",
    "        self.en_hidden_size1 = en_hidden_size1\n",
    "        self.en_hidden_size2 = en_hidden_size2\n",
    "        self.en_score_dim = en_score_dim\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # may useful optimize method\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False # optimize method\n",
    "    warmup_tokens = 375e6 # use this to train model from a lower learning rate\n",
    "    final_tokens = 260e9 # all tokens during whole training process\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'local-likely-model.pt' # save model path\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = ModelConfig(model_name, entity_vector, en_embd_dim, en_hidden_size1=128, \n",
    "                    en_hidden_size2=128, en_score_dim=3, use_en_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # print model structure\n",
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# do not train bert embedding layer\n",
    "for par in model.bert.embeddings.parameters(): \n",
    "    par.requires_grad = False\n",
    "# only train last(11th) bert encode layer\n",
    "for par in model.bert.encoder.layer[:11].parameters(): \n",
    "    par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : all params: 268.147872M\n",
      "Model : need grad params: 7.710796M\n"
     ]
    }
   ],
   "source": [
    "# print model all parameters and parameters need training\n",
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 1\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 6400\n",
      "final_tokens 404672\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=1, learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=32*200, final_tokens=1*batch_size*len(train_dataloader),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 13: train loss 0.65636. score 0.62500. lr 4.200000e-05:   0%|          | 13/12646 [00:05<1:21:21,  2.59it/s]"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "    \n",
    "    def predict(self, text, entities):\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,             \n",
    "                        truncation=True,\n",
    "                        padding = 'max_length',     \n",
    "                        return_tensors = 'pt'       \n",
    "                   ).to(device)\n",
    "        \n",
    "        entity_ids = [entity_to_index.get(entity, entity_to_index[\"<unk>\"]) for entity in entities][:en_pad_size]\n",
    "        for i in range(en_pad_size - len(entity_ids)):\n",
    "            entity_ids.append(entity_to_index[\"<pad>\"])\n",
    "        entity_ids = torch.tensor(entity_ids).unsqueeze(0).to(device)\n",
    "        entity_length = torch.tensor(len(entities)).unsqueeze(0).to(device)    \n",
    "        # build entity score\n",
    "        entity_score = []\n",
    "        score = 1\n",
    "        for en in entities:\n",
    "            if en in entity_score_dict:\n",
    "                en_score = float(entity_score_dict[en])\n",
    "                score *= en_score\n",
    "        score = math.log(10000,10)\n",
    "        entity_score.append(score)\n",
    "        if score >= 0:\n",
    "            entity_score.append(score**2)\n",
    "            entity_score.append(score**0.5)\n",
    "        else:\n",
    "            entity_score.append(-score**2)\n",
    "            entity_score.append(-(abs(score)**0.5))\n",
    "        entity_score = torch.tensor(entity_score).unsqueeze(0).to(device)    \n",
    "        \n",
    "        self.model.eval()\n",
    "        pred = torch.sigmoid(self.model(input_ids, entity_ids, entity_length, entity_score)[0])\n",
    "        return pred.item()\n",
    "    \n",
    "    def count_acc(self, text_list, local):\n",
    "        result = []\n",
    "        for text in text_list:\n",
    "            result.append(self.predict(text))\n",
    "        result = torch.tensor(result, dtype = torch.float)\n",
    "        if local:\n",
    "            acc = sum(result > 0.5).item()/len(result)\n",
    "        else:\n",
    "            acc = sum(result < 0.5).item()/len(result)\n",
    "        return result, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_state_dict(torch.load(\"local-likely-model.pt\", map_location = device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text_list = []\n",
    "test_entity_list = []\n",
    "with open('data/test_data_1k.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        test_text_list.append(line[1])\n",
    "        test_entity_list.append(line[2].split('|'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_predict = []\n",
    "for text, entities in zip(test_text_list,test_entity_list):\n",
    "    prob = predict.predict(text,entities)\n",
    "    model_predict.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "print(len(model_predict))\n",
    "fout = open('model-predict.tsv','w')\n",
    "for prob in model_predict:\n",
    "    fout.write('{}\\n'.format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Keeneland confirms case of COVID at Thoroughbred training center . A stable worker at Keeneland 's training track on Paris Pike tested positive for the coronavirus , the track confirmed . The woman , who is a hot walker -LRB- someone who walks horses as they cool down -RRB- , works at the Thoroughbred Center , said Vince Gabbert , Keeneland vice president and COO . She tested positive about three weeks ago and has recovered and returned to work . One of her family members who also works in the training center barns also self-quarantined . Gabbert said that another stable employee at Keeneland 's Rice Road barn area self-quarantined after a family member tested positive for COVID-19 . In both cases , the employees work for trainers who are part of the Lexington racetrack 's resident barn population and neither had sent horses out of state to race . A small number of horses have shipped to the handful of tracks that are still conducting live racing and returned uneventfully , Gabbert said . `` We 're tracking people shipping in and out , but not much shipping is going on , '' Gabbert said . At The Thoroughbred Center , 18 horses were shipped to Will Rogers Downs in Oklahoma and one horse was sent to Oaklawn Park in Arkansas . No Keeneland or training center employees have tested positive , he said . And there have been no other cases , he said . Keeneland adopted measures in March designed to reduce the risk of transmission and to detect illness , including instituting temperature checks for those coming on the grounds and limiting access to those with business there . Gabbert said that the low number of cases linked to the racetrack show that the measures are working , something that could be key as Keeneland and Churchill Downs request dates for racing this summer . `` We 're following the right protocols and we will continue to do so , he said . Keeneland is planning to hold its crucial September yearling sale as scheduled ; the track also is planning to host the Breeders ' Cup World Championships in early November . On March 16 , Keeneland canceled its entire spring race meet , after initially planning to host the first two weeks without spectators . At the time , the CDC had issued guidance against gatherings with large numbers of spectators for at least eight weeks . Churchill Downs also has postponed the Kentucky Derby , which has been rescheduled for Labor Day weekend .\""
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "index = 31\n",
    "test_text_list[index]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.631829559803009"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.predict(test_text_list[index], test_entity_list[index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
