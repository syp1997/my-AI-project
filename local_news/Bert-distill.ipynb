{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "import transformers\n",
    "from transformers import AutoModel, AutoTokenizer\n",
    "from transformers import get_linear_schedule_with_warmup\n",
    "import csv \n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import math\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up logging\n",
    "import logging\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = 'distilbert-base-uncased'\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataProcess():\n",
    "    \n",
    "    def __init__(self, root, ratio, batch_size):\n",
    "        self.root = root\n",
    "        self.ratio = ratio\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def prepare_data(self):\n",
    "        text_list = []\n",
    "        label_list = []\n",
    "        with open(root, 'r') as f:\n",
    "            reader = csv.reader(f, delimiter='\\t')\n",
    "            for line in reader:\n",
    "                text_list.append(line[0])\n",
    "                label_list.append(int(line[1]))\n",
    "        return text_list, label_list\n",
    "\n",
    "    # Function to get token ids for a list of texts \n",
    "    def encode_data(self, text_id_root, labels_root):\n",
    "        text_list, label_list = self.prepare_data()\n",
    "        all_input_ids = []    \n",
    "        for text in text_list:\n",
    "            input_ids = tokenizer.encode(\n",
    "                            text,                      \n",
    "                            add_special_tokens = True,             \n",
    "                            truncation=True,\n",
    "                            padding = 'max_length',     \n",
    "                            return_tensors = 'pt'       \n",
    "                       )\n",
    "            all_input_ids.append(input_ids)    \n",
    "        all_input_ids = torch.cat(all_input_ids, dim=0)\n",
    "        labels = torch.tensor(label_list, dtype=torch.float)\n",
    "        # Save tensor\n",
    "        torch.save(all_input_ids, text_id_root)\n",
    "        torch.save(labels, labels_root)\n",
    "        return all_input_ids, labels\n",
    "    \n",
    "    def load_data(self, text_id_root, labels_root):\n",
    "        all_input_ids = torch.load(text_id_root)\n",
    "        labels = torch.load(labels_root)\n",
    "        # Split data into train and validation\n",
    "        dataset = TensorDataset(all_input_ids, labels)\n",
    "        train_size = int(self.ratio * len(dataset))\n",
    "        valid_size = len(dataset) - train_size\n",
    "        train_dataset, valid_dataset = random_split(dataset, [train_size, valid_size])\n",
    "\n",
    "        # Create train and validation dataloaders\n",
    "        train_dataloader = DataLoader(train_dataset, batch_size = self.batch_size, shuffle = True)\n",
    "        valid_dataloader = DataLoader(valid_dataset, batch_size = self.batch_size, shuffle = False)\n",
    "\n",
    "        return train_dataloader, valid_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8\n",
    "batch_size = 32\n",
    "root = \"data/all_data_1012.tsv\"\n",
    "text_id_root = \"data/text_id_tensor.pt\"\n",
    "labels_root = \"data/labels_tensor.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcess(root, ratio, batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# all_input_ids, labels = processor.encode_data(text_id_root, labels_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader, valid_dataloader = processor.load_data(text_id_root ,labels_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  4223\n",
      "Num of valid_dataloader:  1056\n"
     ]
    }
   ],
   "source": [
    "print(\"Num of train_dataloader: \", len(train_dataloader))\n",
    "print(\"Num of valid_dataloader: \", len(valid_dataloader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Bert(nn.Module):\n",
    "\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.bert = AutoModel.from_pretrained(config.model_name)\n",
    "        self.hidden_size = self.bert.transformer.layer[-1].output_layer_norm.weight.shape[0]\n",
    "        self.dropout = nn.Dropout(config.dropout_prob)\n",
    "        self.avgpool = torch.nn.AvgPool1d(config.pad_size)\n",
    "        self.fc = nn.Linear(self.hidden_size, config.output_size)\n",
    "        \n",
    "    def configure_optimizers(self, train_config):\n",
    "#         param_optimizer = list(model.named_parameters())  \n",
    "#         no_decay = ['bias', 'LayerNorm.bias', 'LayerNorm.weight']\n",
    "#         optimizer_grouped_parameters = [\n",
    "#                 {'params': [p for n, p in param_optimizer if not any(nd in n for nd in no_decay)], 'weight_decay': train_config.weight_decay},\n",
    "#                 {'params': [p for n, p in param_optimizer if any(nd in n for nd in no_decay)], 'weight_decay': 0.0}]\n",
    "#         optimizer = transformers.AdamW(optimizer_grouped_parameters, lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=train_config.learning_rate, betas=train_config.betas)\n",
    "        return optimizer\n",
    "\n",
    "    def forward(self, input_ids, labels=None, token_type_ids=None, attention_mask=None):\n",
    "        layer_norm_output = self.bert(input_ids, token_type_ids, attention_mask,)[0]\n",
    "        # layer_norm_output: [batch_size, 512, 768]\n",
    "        x = self.dropout(layer_norm_output).transpose(1,2)\n",
    "        x = self.avgpool(x).squeeze(-1)\n",
    "        y_pred = self.fc(x).squeeze(-1)\n",
    "        # y_pred: [batch_size, output_dim]\n",
    "        if labels is not None:\n",
    "            loss = F.binary_cross_entropy_with_logits(y_pred, labels)\n",
    "            return y_pred, loss\n",
    "        else:\n",
    "            return y_pred "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BertConfig:\n",
    "    \"\"\" base GPT config, params common to all GPT versions \"\"\"\n",
    "    dropout_prob = 0.1\n",
    "\n",
    "    def __init__(self, output_size, model_name, **kwargs):\n",
    "        self.output_size = output_size\n",
    "        self.model_name = model_name\n",
    "        for k, v in kwargs.items():\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Trainer:\n",
    "\n",
    "    def __init__(self, model, train_loader, test_loader, config):\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.test_loader = test_loader\n",
    "        self.config = config\n",
    "\n",
    "        # take over whatever gpus are on the system\n",
    "        self.device = 'cpu'\n",
    "        if torch.cuda.is_available():\n",
    "            self.device = torch.cuda.current_device()\n",
    "            self.model = torch.nn.DataParallel(self.model).to(self.device)\n",
    "\n",
    "    def save_checkpoint(self):\n",
    "        # DataParallel wrappers keep raw model object in .module attribute\n",
    "        raw_model = self.model.module if hasattr(self.model, \"module\") else self.model\n",
    "        logger.info(\"saving %s\", self.config.ckpt_path)\n",
    "        torch.save(raw_model.state_dict(), self.config.ckpt_path)\n",
    "        \n",
    "    def binary_accuracy(self, preds, y):\n",
    "        rounded_preds = torch.round(torch.sigmoid(preds))\n",
    "        correct = (rounded_preds == y).float()\n",
    "        acc = correct.sum() / len(correct)\n",
    "        return acc\n",
    "\n",
    "    def train(self):\n",
    "        model, config = self.model, self.config\n",
    "        raw_model = model.module if hasattr(self.model, \"module\") else model\n",
    "        optimizer = raw_model.configure_optimizers(config)\n",
    "\n",
    "        def run_epoch(split):\n",
    "            is_train = split == 'train'\n",
    "            model.train(is_train)\n",
    "            loader = self.train_loader if is_train else self.test_loader\n",
    "            \n",
    "            losses = []\n",
    "            all_y = []\n",
    "            all_y_pred = []\n",
    "            pbar = tqdm(enumerate(loader), total=len(loader)) if is_train else enumerate(loader)\n",
    "            for it, (x, y) in pbar:\n",
    "                # place data on the correct device\n",
    "                x = x.to(self.device)\n",
    "                y = y.to(self.device)\n",
    "                # forward the model\n",
    "                with torch.set_grad_enabled(is_train):\n",
    "                    y_pred, loss = model(x, y)\n",
    "                    loss = loss.mean() # collapse all losses if they are scattered on multiple gpus\n",
    "                    losses.append(loss.item())\n",
    "                    step_score = self.binary_accuracy(y_pred, y)\n",
    "                    all_y.extend(y)\n",
    "                    all_y_pred.extend(y_pred)\n",
    "                \n",
    "                if is_train:\n",
    "\n",
    "                    # backprop and update the parameters\n",
    "                    model.zero_grad()\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), config.grad_norm_clip)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    # decay the learning rate based on our progress\n",
    "                    if config.lr_decay:\n",
    "                        self.tokens += (y >= 0).sum() # number of tokens processed this step (i.e. label is not -100)\n",
    "                        if self.tokens < config.warmup_tokens:\n",
    "                            # linear warmup\n",
    "                            lr_mult = float(self.tokens) / float(max(1, config.warmup_tokens))\n",
    "                        else:\n",
    "                            # cosine learning rate decay\n",
    "                            progress = float(self.tokens - config.warmup_tokens) / float(max(1, config.final_tokens - config.warmup_tokens))\n",
    "                            lr_mult = max(0.1, 0.5 * (1.0 + math.cos(math.pi * progress)))\n",
    "                        lr = config.learning_rate * lr_mult\n",
    "                        for param_group in optimizer.param_groups:\n",
    "                            param_group['lr'] = lr\n",
    "                    else:\n",
    "                        lr = config.learning_rate\n",
    "\n",
    "                    # report progress\n",
    "                    pbar.set_description(f\"epoch {epoch+1} iter {it}: train loss {loss.item():.5f}. score {step_score:.5f}. lr {lr:e}\")\n",
    "\n",
    "            if not is_train:\n",
    "                test_loss = float(np.mean(losses))\n",
    "                all_y = torch.stack(all_y, dim=0)\n",
    "                all_y_pred = torch.stack(all_y_pred, dim=0)\n",
    "                test_score = self.binary_accuracy(all_y_pred, all_y)\n",
    "                logger.info(\"test loss: %f\", test_loss)\n",
    "                logger.info(\"test score: %f\", test_score)\n",
    "                return test_loss\n",
    "\n",
    "        self.tokens = 0 # counter used for learning rate decay\n",
    "        best_loss = float('inf')\n",
    "#         best_loss = run_epoch('test')\n",
    "        for epoch in range(config.max_epochs):\n",
    "\n",
    "            run_epoch('train')\n",
    "            if self.test_loader is not None:\n",
    "                test_loss = run_epoch('test')\n",
    "\n",
    "            # supports early stopping based on the test loss, or just save always if no test set is provided\n",
    "            good_model = self.test_loader is None or test_loss < best_loss\n",
    "            if self.config.ckpt_path is not None and good_model:\n",
    "                best_loss = test_loss\n",
    "                self.save_checkpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrainerConfig:\n",
    "    # optimization parameters\n",
    "    max_epochs = 10\n",
    "    learning_rate = 3e-4\n",
    "    betas = (0.9, 0.95)\n",
    "    grad_norm_clip = 1.0\n",
    "    weight_decay = 0.1 # only applied on matmul weights\n",
    "    # learning rate decay params: linear warmup followed by cosine decay to 10% of original\n",
    "    lr_decay = False\n",
    "    warmup_tokens = 375e6 # these two numbers come from the GPT-3 paper, but may not be good defaults elsewhere\n",
    "    final_tokens = 260e9 # (at what point we reach 10% of original LR)\n",
    "    # checkpoint settings\n",
    "    ckpt_path = 'bert-distill-model.pt'\n",
    "    num_workers = 0 # for DataLoader\n",
    "\n",
    "    def __init__(self, **kwargs):\n",
    "        for k,v in kwargs.items():\n",
    "            print(k,v)\n",
    "            setattr(self, k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_size = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = BertConfig(output_size, model_name, pad_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Bert(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for par in model.bert.embeddings.parameters():\n",
    "    par.requires_grad = False\n",
    "for par in model.bert.transformer.layer[:5].parameters():\n",
    "    par.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bert : all params: 66.363649M\n",
      "Bert : need grad params: 7.088641M\n"
     ]
    }
   ],
   "source": [
    "print('{} : all params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters()) / 1000 / 1000))\n",
    "print('{} : need grad params: {:4f}M'.format(model._get_name(), sum(p.numel() for p in model.parameters() if p.requires_grad) / 1000 / 1000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert-distill-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 2\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 6400\n",
      "final_tokens 270272\n",
      "num_workers 1\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=2, learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=32*200, final_tokens=2*batch_size*len(train_dataloader),\n",
    "                      num_workers=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 4222: train loss 0.04074. score 1.00000. lr 3.115017e-04: 100%|██████████| 4223/4223 [38:33<00:00,  1.83it/s]\n",
      "10/15/2020 15:51:39 - test loss: 0.077776\n",
      "10/15/2020 15:51:39 - test score: 0.972379\n",
      "10/15/2020 15:51:39 - saving bert-distill-model.pt\n",
      "epoch 2 iter 4222: train loss 0.01281. score 1.00000. lr 6.000000e-05: 100%|██████████| 4223/4223 [44:05<00:00,  1.60it/s]\n",
      "10/15/2020 16:41:49 - test loss: 0.074564\n",
      "10/15/2020 16:41:49 - test score: 0.974896\n",
      "10/15/2020 16:41:49 - saving bert-distill-model.pt\n"
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predict:\n",
    "    \n",
    "    def __init__(self, model):\n",
    "        self.model = model.to(device)\n",
    "    \n",
    "    def predict(self, text):\n",
    "        input_ids = tokenizer.encode(\n",
    "                        text,                      \n",
    "                        add_special_tokens = True,             \n",
    "                        truncation=True,\n",
    "                        padding = 'max_length',     \n",
    "                        return_tensors = 'pt'       \n",
    "                   ).to(device)\n",
    "        self.model.eval()\n",
    "        pred = torch.sigmoid(self.model(input_ids)[0])\n",
    "        return pred.item()\n",
    "    \n",
    "    def count_acc(self, text_list, local):\n",
    "        result = []\n",
    "        for text in text_list:\n",
    "            result.append(self.predict(text))\n",
    "        result = torch.tensor(result, dtype = torch.float)\n",
    "        if local:\n",
    "            acc = sum(result > 0.5).item()/len(result)\n",
    "        else:\n",
    "            acc = sum(result < 0.5).item()/len(result)\n",
    "        return result, acc\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"bert-distill-model.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = Predict(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_text = []\n",
    "with open('data/test_data_1k.tsv') as f:\n",
    "    reader= csv.reader(f, delimiter='\\t')\n",
    "    for line in reader:\n",
    "        test_text.append(line[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "bert_distill_predict = []\n",
    "for text in test_text:\n",
    "    prob = predict.predict(text)\n",
    "    bert_distill_predict.append(prob)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "999\n"
     ]
    }
   ],
   "source": [
    "fout = open('bert-distill-predict.tsv','w')\n",
    "print(len(bert_distill_predict))\n",
    "for prob in bert_distill_predict:\n",
    "    fout.write('{}\\n'.format(prob))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Swift , Apple 's open source programming language , announces Swift Algorithms . Swift has announced Swift Algorithms . The announcement was made in a blog post on the Swift.org website . The algorithms should help developers fix code and improve app performance faster . Announced on Twitter on Wednesday night , Swift , Apple 's open-source programming language , is bringing new algorithm packages to developers . The announcement was made through the Swift Language account : The new open source Swift Algorithms package was just released , and ready for the community to jump in ! The new open source Swift Algorithms package was just released , and ready for the community to jump in ! To learn more about these new sequence and collection-focused algorithms , head to the https://t.co/5NNXraGyus blog : https://t.co/EsoUq1Q0pU -- Swift Language -LRB- @SwiftLang -RRB- October 8 , 2020 In a blog post on the Swift website , Nate Cook , a member of the Swift standard library team at Apple , says that the team hopes developers will adopt the new algorithms to help assist in correcting their code . `` I 'm excited to announce Swift Algorithms , a new open-source package of sequence and collection algorithms , along with their related types . Algorithms are powerful tools for thought because they encapsulate difficult-to-read and error-prone raw loops . The Algorithms package includes a host of powerful , generic algorithms frequently found in other popular programming languages . We hope this new package will help people embrace algorithms , improving the correctness and performance of their code . '' Developers can check out Swift Algorithms on the Swift website .\""
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text = test_text[6]\n",
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.056410130113363266"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predict.predict(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow2.0-pytorch1.4",
   "language": "python",
   "name": "tensorflow2.0-pytorch1.4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
