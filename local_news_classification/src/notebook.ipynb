{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from transformers import BertTokenizer\n",
    "from wikipedia2vec import Wikipedia2Vec\n",
    "import time\n",
    "import csv\n",
    "import warnings\n",
    "import logging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import *\n",
    "from data_processor import DataProcess\n",
    "from model import Model, ModelConfig\n",
    "from trainer import Trainer, TrainerConfig\n",
    "from test_data_collator import TestDataCollator\n",
    "from predictor import Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    ")\n",
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained bert model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use pretrained wiki_vector model\n",
    "model_file = '/data/suyinpei/wiki_vector.model'\n",
    "wiki2vec = Wikipedia2Vec.load(model_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ratio = 0.8 # ratio of train data to valid data\n",
    "batch_size = 32 # batch size\n",
    "en_pad_size = 12 # max entity number of one data\n",
    "en_embd_dim = 100 # entity embedding dim\n",
    "idf_file = '/data/suyinpei/idf_bigram5.txt'\n",
    "entity_frep_file = '/data/suyinpei/entity_frep.tsv'\n",
    "data_root = \"/data/suyinpei/all_data_1028.tsv\" # data: docid, text, entities, label\n",
    "text_id_root = \"/data/suyinpei/text_ids_1028.pt\" # data_size * 512\n",
    "labels_root = \"/data/suyinpei/labels_1028.pt\" # data_size\n",
    "entity_id_root = \"/data/suyinpei/entity_ids_1028.pt\" # data_size * 12\n",
    "entity_length_root = \"/data/suyinpei/entity_length_1028.pt\" # data_size\n",
    "entity_score_root = \"/data/suyinpei/entity_score_1028.pt\" # data_size * 3\n",
    "entity_vector_root = \"/data/suyinpei/entity_vectors_1028.pt\" # en_vocab_size * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = DataProcess(data_root, text_id_root, labels_root, entity_id_root, entity_length_root, entity_score_root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this when using new data, build text index and label\n",
    "# all_input_ids, labels = processor.encode_text(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All Entity number:  7744598\n",
      "Entity vocab size:  1600870\n"
     ]
    }
   ],
   "source": [
    "# get entity vocab for predict\n",
    "entity_to_index, index_to_entity = processor.build_entity_vocab()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# run this when need compute entity vector\n",
    "idf_dict, unk_idf = processor.load_idf(idf_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # run this when use new data, build entity vector and index\n",
    "# build_entity_vector = processor.build_entity_vector(entity_to_index, index_to_entity, wiki2vec, idf_dict, unk_idf, en_embd_dim, entity_vector_root)\n",
    "# all_entity_ids, all_entity_length = processor.build_entity_id(entity_to_index, index_to_entity, en_pad_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity Score vocab size:  667095\n"
     ]
    }
   ],
   "source": [
    "entity_score_dict = processor.load_entity_score_dict(entity_frep_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity score mean:  tensor([[0.4746, 8.1614, 0.2299]])\n",
      "Entity score std:  tensor([[  6.5568, 219.4999,   2.1919]])\n"
     ]
    }
   ],
   "source": [
    "# get entity score mean and std\n",
    "_, entity_score_mean, entity_score_std = processor.build_entity_score(entity_score_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity vector shape:  torch.Size([1600870, 100])\n"
     ]
    }
   ],
   "source": [
    "entity_vector = processor.load_entity_vector(entity_vector_root) # get pretrained entity_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num of train_dataloader:  12715\n",
      "Num of valid_dataloader:  3179\n"
     ]
    }
   ],
   "source": [
    "train_dataloader, valid_dataloader = processor.load_data(ratio, batch_size) # build train/valid dataloader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "mconf = ModelConfig(model_name, entity_vector, en_embd_dim, en_hidden_size1=128, \n",
    "                    en_hidden_size2=128, en_score_dim=3, use_en_encoder=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Model(mconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model : all params: 269.601572M\n",
      "Model : need grad params: 7.710796M\n"
     ]
    }
   ],
   "source": [
    "model.fix_layer_grad()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_epochs 1\n",
      "learning_rate 0.0006\n",
      "lr_decay True\n",
      "warmup_tokens 6400\n",
      "final_tokens 406880\n",
      "num_workers 1\n",
      "ckpt_path ../models/local-likely-model.pt\n"
     ]
    }
   ],
   "source": [
    "tconf = TrainerConfig(max_epochs=1, learning_rate=6e-4, lr_decay=True, \n",
    "                      warmup_tokens=32*200, final_tokens=1*batch_size*len(train_dataloader),\n",
    "                      num_workers=1, ckpt_path='../models/local-likely-model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device: cuda\n"
     ]
    }
   ],
   "source": [
    "trainer = Trainer(model, train_dataloader, valid_dataloader, tconf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1 iter 12714: train loss 0.00613. score 1.00000. lr 6.000000e-05: 100%|██████████| 12715/12715 [1:23:01<00:00,  2.55it/s]\n"
     ]
    }
   ],
   "source": [
    "# start training\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test data collect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#load model\n",
    "model.load_state_dict(torch.load('../models/local-likely-model-iter10.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_file = \"/data/suyinpei/test_data_1k.tsv\"\n",
    "test_batch = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data_collator = TestDataCollator(test_data_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encode text: Took 10.463449954986572 seconds\n",
      "Encode entity: Took 2.084402322769165 seconds\n"
     ]
    }
   ],
   "source": [
    "test_dataloader = test_data_collator.load_data(test_batch, tokenizer, entity_to_index, index_to_entity, wiki2vec, idf_dict, unk_idf, \n",
    "                                    en_pad_size, en_embd_dim, entity_score_dict, entity_score_mean, entity_score_std)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "use device: cuda\n"
     ]
    }
   ],
   "source": [
    "predictor = Predictor(model, test_dataloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test Progress: 100%|██████████| 32/32 [00:09<00:00,  3.23it/s]\n",
      "10/30/2020 08:41:56 - Took 9.923027276992798 seconds\n"
     ]
    }
   ],
   "source": [
    "last_time = time.time()\n",
    "model_predict = predictor.predict()\n",
    "logger.info('Took {} seconds'.format(time.time() - last_time))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "10/30/2020 08:41:56 - Predict number: 999\n"
     ]
    }
   ],
   "source": [
    "logger.info('Predict number: {}'.format(model_predict.shape[0]))\n",
    "fout = open('../data/model-predict.tsv','w')\n",
    "for prob in model_predict:\n",
    "    fout.write('{}\\n'.format(prob.item()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
